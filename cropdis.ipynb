{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cropdis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBpErQXrozqxL72oHTc8iZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreejajulakanti/Stock-Market/blob/main/cropdis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RzKf5O72TbB"
      },
      "outputs": [],
      "source": [
        "pip install opencv-python\n",
        "pip install mahotas\n",
        "#-----------------------------------\n",
        "# GLOBAL FEATURE EXTRACTION\n",
        "#-----------------------------------\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import mahotas\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n",
        "\n",
        "#--------------------\n",
        "# tunable-parameters\n",
        "#--------------------\n",
        "images_per_class       = 800\n",
        "fixed_size             = tuple((500, 500))\n",
        "train_path             = \"dataset/train\"\n",
        "h5_train_data          = 'output/train_data.h5'\n",
        "h5_train_labels        = 'output/train_labels.h5'\n",
        "bins                   = 8\n",
        "# Converting each image to RGB from BGR format\n",
        "\n",
        "def rgb_bgr(image):\n",
        "    rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return rgb_img\n",
        "# Conversion to HSV image format from RGB\n",
        "\n",
        "def bgr_hsv(rgb_img):\n",
        "    hsv_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2HSV)\n",
        "    return hsv_img\n",
        "# image segmentation\n",
        "\n",
        "# for extraction of green and brown color\n",
        "\n",
        "\n",
        "def img_segmentation(rgb_img,hsv_img):\n",
        "    lower_green = np.array([25,0,20])\n",
        "    upper_green = np.array([100,255,255])\n",
        "    healthy_mask = cv2.inRange(hsv_img, lower_green, upper_green)\n",
        "    result = cv2.bitwise_and(rgb_img,rgb_img, mask=healthy_mask)\n",
        "    lower_brown = np.array([10,0,10])\n",
        "    upper_brown = np.array([30,255,255])\n",
        "    disease_mask = cv2.inRange(hsv_img, lower_brown, upper_brown)\n",
        "    disease_result = cv2.bitwise_and(rgb_img, rgb_img, mask=disease_mask)\n",
        "    final_mask = healthy_mask + disease_mask\n",
        "    final_result = cv2.bitwise_and(rgb_img, rgb_img, mask=final_mask)\n",
        "    return final_result\n",
        "# feature-descriptor-2: Haralick Texture\n",
        "def fd_haralick(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
        "    return haralick\n",
        "  # feature-descriptor-3: Color Histogram\n",
        "def fd_histogram(image, mask=None):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
        "    cv2.normalize(hist, hist)\n",
        "    return hist.flatten()\n",
        " # get the training labels\n",
        "train_labels = os.listdir(train_path)\n",
        "\n",
        "# sort the training labels\n",
        "train_labels.sort()\n",
        "print(train_labels)\n",
        "\n",
        "# empty lists to hold feature vectors and labels\n",
        "global_features = []\n",
        "labels          = []\n",
        "# loop over the training data sub-folders\n",
        "for training_name in train_labels:\n",
        "    # join the training data path and each species training folder\n",
        "    dir = os.path.join(train_path, training_name)\n",
        "\n",
        "    # get the current training label\n",
        "    current_label = training_name\n",
        "\n",
        "    # loop over the images in each sub-folder\n",
        "    for x in range(1,images_per_class+1):\n",
        "        # get the image file name\n",
        "        file = dir + \"/\" + str(x) + \".jpg\"\n",
        "\n",
        "        # read the image and resize it to a fixed-size\n",
        "        image = cv2.imread(file)\n",
        "        image = cv2.resize(image, fixed_size)\n",
        "\n",
        "        \n",
        "        # Running Function Bit By Bit\n",
        "        \n",
        "        RGB_BGR       = rgb_bgr(image)\n",
        "        BGR_HSV       = bgr_hsv(RGB_BGR)\n",
        "        IMG_SEGMENT   = img_segmentation(RGB_BGR,BGR_HSV)\n",
        "\n",
        "        # Call for Global Fetaure Descriptors\n",
        "        \n",
        "        fv_hu_moments = fd_hu_moments(IMG_SEGMENT)\n",
        "        fv_haralick   = fd_haralick(IMG_SEGMENT)\n",
        "        fv_histogram  = fd_histogram(IMG_SEGMENT)\n",
        "        \n",
        "        # Concatenate \n",
        "        \n",
        "        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
        "        \n",
        "        \n",
        "\n",
        "        # update the list of labels and feature vectors\n",
        "        labels.append(current_label)\n",
        "        global_features.append(global_feature)\n",
        "\n",
        "    print(\"[STATUS] processed folder: {}\".format(current_label))\n",
        "\n",
        "print(\"[STATUS] completed Global Feature Extraction...\")\n",
        "print(\"[STATUS] feature vector size {}\".format(np.array(global_features).shape))\n",
        "# get the overall training label size\n",
        "print(\"[STATUS] training Labels {}\".format(np.array(labels).shape))\n",
        "# encode the target labels\n",
        "targetNames = np.unique(labels)\n",
        "le          = LabelEncoder()\n",
        "target      = le.fit_transform(labels)\n",
        "print(\"[STATUS] training labels encoded...\")\n",
        "# scale features in the range (0-1)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler            = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaled_features = scaler.fit_transform(global_features)\n",
        "print(\"[STATUS] feature vector normalized...\")\n",
        "print(\"[STATUS] target labels: {}\".format(target))\n",
        "print(\"[STATUS] target labels shape: {}\".format(target.shape))\n",
        "# save the feature vector using HDF5\n",
        "h5f_data = h5py.File(h5_train_data, 'w')\n",
        "h5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))\n",
        "h5f_label = h5py.File(h5_train_labels, 'w')\n",
        "h5f_label.create_dataset('dataset_1', data=np.array(target))\n",
        "h5f_data.close()\n",
        "h5f_label.close()\n",
        "# split the training and testing data\n",
        "(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),\n",
        "                                                                                          np.array(global_labels),\n",
        "                                                                                          test_size=test_size,\n",
        "                                                                                          random_state=seed)\n",
        "\n",
        "print(\"[STATUS] splitted train and test data...\")\n",
        "print(\"Train data  : {}\".format(trainDataGlobal.shape))\n",
        "print(\"Test data   : {}\".format(testDataGlobal.shape)) \n",
        "trainDataGlobal\n",
        "# 10-fold cross validation\n",
        "for name, model in models:\n",
        "    kfold = KFold(n_splits=10, random_state=seed,shuffle=True)\n",
        "    cv_results = cross_val_score(model, trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Machine Learning algorithm comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()\n",
        "clf  = RandomForestClassifier(n_estimators=num_trees, random_state=seed)\n",
        "clf.fit(trainDataGlobal, trainLabelsGlobal)\n",
        "y_predict=clf.predict(testDataGlobal)\n",
        "y_predict\n",
        "cm = confusion_matrix(testLabelsGlobal,y_predict)\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm ,annot=True)\n",
        "print(classification_report(testLabelsGlobal,y_predict))\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(testLabelsGlobal, y_predict)"
      ]
    }
  ]
}